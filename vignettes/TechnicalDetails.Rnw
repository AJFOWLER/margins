%\VignetteEngine{knitr}
%\VignetteIndexEntry{Technical Implementation Details}
\documentclass{article}

\title{Interpreting Regression Results using Marginal Effects with R's \textbf{margins}}
\author{Thomas J. Leeper}

\begin{document}

\maketitle

{\abstract Applied data analysts regularly need to make use of regression analysis to understand descriptive, predictive, and causal patterns in data. While many applications of ordinary least squares yield estimated regression coefficients that are readily interpretable as the predicted change in $y$ due to a unit change in $x$, models that involve multiplicative interactions or other complex terms are subjective to less clarity of interpretation. Generalized linear models that involve transformations of this linear predictor into binary, ordinal, count or other discrete outcomes lack such ready interpretation. As such, there has been much debate in the literature about how best to interpret these more complex models (e.g., what quantities of interest to extract? what types of graphical presentations to use?). This article proposes that marginal effects, specifically average marginal effects, provide a unified and intuitive way of describing relationships estimated with regression. To begin, I briefly discuss the challenges of interpreting complex models and review existing views on how to interpret such models, before describing average marginal effects and the somewhat challenging computational task of extracting this quantity of interest from regression results. I conclude with implications for statistical practice and for the design of statistical software.}

The quantity of interest typically reported by statistical software estimation commands for regression models is the regression coefficient, along with standard errors thereof, and various goodness-of-fit and summary statistics. This default output makes sense for additive linear models (i.e., ordinary least squares regression) because an estimated coefficient is readily and directly interpretable as the expected change in $y$ given a unit change in $x$, holding all other terms constant. When models specify other kinds of terms (e.g., multiple higher powers of a given variable, log transformations, or interactions between variables), the coefficients on those variables do not and cannot clearly communicate the influence of a given righthand-side variable on the outcome because the influence of a given variable is captured by multiple estimated coefficients. Looking at a single estimated coefficient and treating it as a marginal effect leads to numerous problematic interpreations \citep{BramborClarkGolder}.

Furthermore, when models involve a non-linear transformation (e.g., generalized linear models such as logit or probit), the coefficients are typically not directly interpretable at all (even when no power terms, interactions, or other complex terms are included). This is because the coefficients express the influence of each separate variable onto the latent, linear scale of the outcome, not the discrete (or probability) scale of the observed outcome. For example, in a logistic regression, the coefficients express the marginal effect of each included variable in terms of the change in log-odds that the outcome equals 1 given a unit change in the independent variable. In order to express the more intuitive change in the predicted probability that the outcome equals 1 requires conditioning on all other included variables (i.e., selecting a set of values for all righthand-side variables) and running that set of values through the link function to convert log-odds to probabilities, thus making the marginal effect (in probability terms) of one variable a function of all other variables included in the model. As such, for both OLS and GLMs, the coefficients estimated for a regression model can often provide unintuitive insight into the statistical relationships between variables and, worse, can frequently fail to communicate those relationships at all (as in GLMs, where the size of coefficients can be completely uninformative about the size of the ``effect'' of a given righthand-side variable).

For these reasons, and in the interest of making intuitive tabular and visual displays of regression results, there is a growing interest in the display of substantively meaningful quantities of interest that can be drawn from regression estimates. Among these, fitted (predicted) values communicate the shape and position of the fitted regression surface (or line in a simple bivariate regression) across the possibly multidimensional covariate space. Predicted values communicate what outcome value would be expected given the patterns observed between covariates and the outcome, enabling both descriptive and predictive inferences. Of more causal relevance is the \textit{marginal effect} of a given variable --- that is, the slope of the regression surface with respect to a given covariate. The marginal effect communicates the rate at which $y$ changes at a given point in covariate space, with respect to one covariate dimension and holding all covariate values constant. This quantity is particularly useful because it is intuitive --- it is simply a slope --- and because it can be calculated from essentially any set of regression estimates.

To calculate marginal effects requires the application of partial derivatives. A marginal effect is, in essence, the slope of multi-dimensional surface with respect to one dimension of that surface. In simple cases, calculation of a marginal effect simply requires application of familiar derivation rules to a regression equation. For generalized linear models involving a link transformation, it further requires the chain rule. This derivation typically requires manual labor (i.e., defining particular partial derivatives and then calculating them anew, by hand, for each estimated model). Thus while the marginal effect is a useful quantity for communicating the substantive influence of a righthand-side variable, it is difficult or at least unintuitive to formulate mentally. This complexity also means that it can be computationally challenging to define and calculate marginal effects for any general set of model specifications. While one can easily write code to calculate the marginal effect of $x1$ for the common regression equation $y = \alpha + \beta_1 x_1 + \beta_2 x_2 + \beta_3 x_1 x_2$ as $\dfrac{\partial y}{x_1} = \beta_1 + \beta_3 x_2$, writing generic code that can calculate $\dfrac{\partial y}{x_1}$ for any arbitrary model specification is far more complicated.

In response to these two factors --- (1) the disconnect between coefficients and substantive influence, and (2) the challenge of conceptualizing and calculating more meaningful quantities of interest --- this article discusses the computational approach of numerical differentiation, an approximation of the partial derivatives of a regression equation and offers the \textbf{margins} package for R as a general application of this approach. The package takes its name and general approach from the \texttt{margins} module introduced in Stata 11 [CITATION], which is a closed-source tool for calculating such effects. The package is also inspired by Clarify (and the R implementation thereof as part of Zelig; [CITATIONS]), which has similar ambitions to convert regression results into meaningful quantities of interest but takes a simulation-based approach different from the one used in \textbf{margins}. Section \ref{sec:clarify} compares the two packages in greater detail.

The outline of this text is as follows: section \ref{sec:stats} describes the statistical background of regression estimation and the distinctions between estimated coefficients and estimated marginal effects of righthand-side variables, Section \ref{sec:quantities} describes the various quantities of interest that might be useful for interpreting regression results, Section \ref{sec:details} describes the computational implementation of \textbf{margins} used to obtain those quantities of interest, and Section \ref{sec:clarify} compares the results of the package to those produced by Stata's \texttt{margins} command and Zelig/Clarify.

\section{Statistical Background}\label{sec:stats}


% 




\section{Quantities of Interest}\label{sec:quantities}

% Fitted values


% use 3d plot in OLS to show relationship between fitted values and marginal effects w/ interaction

% AMEs versus MEMs versus MERs


\section{Computation Details}\label{sec:details}

This section describes the basic computational features of \textbf{margins}. Specifically, it describes the procedures for calculating marginal effects from the information stored in a model object (e.g., an R object of class \texttt{"lm"}) and the procedures for estimating the variances of those marginal effects.


While it is possible to manually differentiate almost any equation using a set of rules, this approach is not particularly easy to implement computationally. Essentially it would require scripting the approach used by a student in an introductory calculus course: in effect deriving a regression formula by hand (i.e., symbolically) using a set of predefined rules (e.g., that built in to R's symbolic derivation table). While feasible, this approach lacks generality because it requires a symbolic derivatives table (or set of rules) that can encompass any model that can be expressed. R's modelling language --- the ``formula'' class --- in fact is sufficiently general (mostly because of support for \texttt{I()} expressions that allow on-the-fly variable transformations) as to make this approach impossible.

% At table, a ala Golder, of symbolic derivatives of formulae


What then can be done? As noted in Section \ref{sec:stats}, the estimated marginal effect of $x_1$ is simply the derivative of the estimated regression equation with respect to $x_1$. Stata's \texttt{margins} command achieves this through numeric derivatives (i.e., numeric approximations of this derivation). \textbf{margins} follows this approach.

What is a numerical approximation? Rather than defining a partial derivative as an exact formula, a numerical approximation approximates the slope of the response function from a model by taking small steps, $h$ in $x$, calculating the $\hat{y}$ at each point, and then applying a simple difference method to define the slope at point  $x$:

\begin{equation}
\dfrac{f(x + h) - f(x)}{h}
\end{equation}

While seemingly crude, as $h \lim 0$, the result converges on the true partial derivative.

% Delta method variances

As in Stata, the \textbf{margins} package approximates the variances of marginal effects using the delta method, which is a linear approximation of the derivative of a set of nested functions.



\section{Comparison with Stata \texttt{margins} and Clarify}\label{sec:clarify}

\subsection{Stata's \texttt{margins} module}


\subsection{Zelig/Clarify}


\section{Conclusion}

Average marginal effects offer an intuitive technique for interpreting regression estimates from a wide class of linear and generalized linear models. While Stata has offered a simple and general computational interface for extracting these quantities of interest from regression models for many years, the approach has not been widely available in other statistical software. The \textbf{margins} port to R makes this functionality much more widely available. By describing the computational approach used by both packages, this article offers users of other languages guidelines for how to apply the approach elsewhere while offering applied data analysts a straightforward explanation of the marginal effect quantity and its derivation.

At present, \textbf{margins} estimates quantities of interest for a wide array of model formulae used in least squares regression and many common generalized linear models. Stata's \texttt{margins} and Zelig/Clarify produce quantities of interest for a wider array of model types. Extension of \textbf{margins} to other model types is planned for the future. The creation of the core \texttt{margins} function as an S3 generic means that the package is easily extensible to other model types (e.g., those introduced in other user-created packages). Development of \textbf{margins} is handled on GitHub, allowing for easy contribution of bug fixes, enhancements, and additional model-specific methods. By publishing \textbf{margins} as free and open-source software (FOSS), it should be straightforward for users of other languages (Python, julia, etc.) to implement similar functionality. Indeed, the port of closed source statistical software to open source represents an underappreciated by critical step in making FOSS data analysis more accessible to those used to working with closed source products.

For applied data analysis, the most important feature of \textbf{margins} is its intuitive use and the near-direct translation of Stata code into R. For those used to Stata's \texttt{margins} command, R's \textbf{margins} package should be a painless transition. For R users not accustomed to calculating marginal effects, \textbf{margins} should also offer a straightforward and tidy way of calculating predicted values and marginal effects, and displaying the results thereof.

\end{document}
